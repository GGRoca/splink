# Pipeline de Prepara√ß√£o de Dados - Base VOD

## Contexto Geral

Este pipeline faz parte de um projeto maior de deduplica√ß√£o e matching de cat√°logos de VOD (Video on Demand). O objetivo final √© construir uma base consolidada anual (t1) com:
- Obras deduplicadas internamente
- Enriquecimento via matching com base Ancine (CPBs)
- Rastreamento temporal (colunas IN/OUT por obra)
- Integra√ß√£o com base hist√≥rica do ano anterior (t0)

**Situa√ß√£o atual:**
- **Base t0** (ano passado): J√° existe, em formato XLSX, j√° possui IN/OUT e matching com CPBs
- **Base t1** (este ano): Em constru√ß√£o - estes notebooks preparam os dados hist√≥ricos
- **Snapshots semanais**: Julho 2024 - Agosto 2025

**Importante:** Os notebooks 1-3 documentados aqui eram originalmente uma prepara√ß√£o para processos de deduplica√ß√£o com Splink (notebooks 4-5, n√£o documentados). Essa abordagem foi abandonada devido a limita√ß√µes t√©cnicas. A solu√ß√£o atual de matching √© determin√≠stica e est√° documentada em `pipeline_cpb_uid.md`, trabalhando apenas com o snapshot mais recente.

---

## Objetivo dos Notebooks 1-3

Estes notebooks preparam o hist√≥rico semanal de cat√°logos, realizando:
1. Convers√£o de formato (XLSX ‚Üí Parquet eficiente)
2. Valida√ß√£o de qualidade e consist√™ncia temporal
3. Consolida√ß√£o e filtragem de plataformas relevantes
4. Prepara√ß√£o de tabela dimens√£o com perfis √∫nicos

**O que falta (a ser implementado):**
- Crunch temporal para gerar colunas IN/OUT com toler√¢ncia de 22 dias
- Integra√ß√£o com base t0
- Expans√£o do XLSX mais recente com obras que sa√≠ram de cat√°logo

---

## Estrutura de Diret√≥rios

```
projeto/
‚îú‚îÄ‚îÄ dados_catalogo/
‚îÇ   ‚îú‚îÄ‚îÄ raw_xlsx/              # Arquivos Excel originais (input)
‚îÇ   ‚îú‚îÄ‚îÄ staging/               # Parquets convertidos (intermedi√°rio)
‚îÇ   ‚îú‚îÄ‚îÄ processed/             # Dados consolidados e limpos (output)
‚îÇ   ‚îî‚îÄ‚îÄ logs/                  # Relat√≥rios de execu√ß√£o
‚îú‚îÄ‚îÄ mapeamento_colunas.json    # Configura√ß√£o de colunas a importar
‚îú‚îÄ‚îÄ plataformas_controle.xlsx  # Lista de plataformas v√°lidas (input manual)
‚îú‚îÄ‚îÄ processador.py             # M√≥dulo auxiliar de convers√£o
‚îú‚îÄ‚îÄ pipeline_cpb_uid.md        # Documenta√ß√£o do matching atual (SEPARADO)
‚îî‚îÄ‚îÄ [notebooks 1, 1.1, 2, 3]   # Pipeline de prepara√ß√£o hist√≥rica
```

**‚ö†Ô∏è Importante:** Os outputs dos notebooks 1-3 **n√£o est√£o sendo usados** no processo de matching atual (documentado em `pipeline_cpb_uid.md`). A solu√ß√£o atual trabalha diretamente com o XLSX mais recente, sem usar o hist√≥rico em Parquet.

---

## Fluxo do Pipeline

```
[1] Convers√£o XLSX ‚Üí Parquet
         ‚Üì
[1.1] An√°lise de Plataformas (opcional/auxiliar)
         ‚Üì
[2] Valida√ß√£o e Filtragem
         ‚Üì
[3] Prepara√ß√£o da Tabela Dimens√£o
         ‚Üì
    OUTPUT FINAL
```

---

## NOTEBOOK 1: Convers√£o XLSX ‚Üí Parquet

### Arquivo: `1_Convers√£o_xlsx-parquet_otimizado.ipynb`

### Objetivo
Converter arquivos Excel semanais para formato Parquet otimizado, aplicando mapeamento de colunas e processamento paralelo.

### Inputs
- **Arquivos**: `dados_catalogo/raw_xlsx/*.xlsx`
  - Nomenclatura: `BB Media - YYYY-MM-DD.xlsx` (data = semana de refer√™ncia)
- **Configura√ß√£o**: `mapeamento_colunas.json`
  - Define quais colunas importar de cada arquivo
  - Evita carregar dados desnecess√°rios

### Depend√™ncias
- **M√≥dulo externo**: `processador.py` (fun√ß√µes de convers√£o)
- Bibliotecas: pandas, openpyxl, pyarrow, pathlib, multiprocessing, tqdm

### Processamento
1. Descobre arquivos `.xlsx` em `raw_xlsx/`
2. Verifica manifesto para evitar reprocessamento
3. **Processamento paralelo** (utiliza todos os n√∫cleos CPU dispon√≠veis)
4. Para cada arquivo:
   - L√™ apenas colunas mapeadas
   - Extrai `data_ref` do nome do arquivo (formato YYYY-MM-DD)
   - Salva como Parquet com compress√£o Snappy

### Outputs
- **Parquets**: `dados_catalogo/staging/BB_Media_YYYY-MM-DD.parquet`
- **Manifesto**: `dados_catalogo/manifesto_xlsx.json`
  - Controle de arquivos j√° processados (evita duplica√ß√£o)
- **Relat√≥rio**: `dados_catalogo/logs/relatorio_conversao_TIMESTAMP.csv`
  - Status de cada arquivo (sucesso/erro)
  - Colunas n√£o mapeadas encontradas

### Caracter√≠sticas
- **Incremental**: S√≥ processa arquivos novos (usa manifesto)
- **Performance**: Multiprocessing para processamento em lote
- **Valida√ß√£o**: Identifica colunas n√£o mapeadas (para atualizar configura√ß√£o)

---

## NOTEBOOK 1.1: Hist√≥rico de Plataformas (AUXILIAR)

### Arquivo: `1_1historico_de_plataformas.ipynb`

### Objetivo
An√°lise explorat√≥ria do hist√≥rico de plataformas presentes nos dados. **N√£o √© essencial para o fluxo principal** - serve como ferramenta de valida√ß√£o e entendimento dos dados.

### Input
- **Parquets**: `dados_catalogo/staging/*.parquet`

### Depend√™ncias
- Bibliotecas: pandas, pathlib, duckdb

### Processamento
- Usa DuckDB para agrega√ß√£o r√°pida
- Para cada plataforma, calcula:
  - Total de registros
  - Semanas presentes
  - Primeira e √∫ltima apari√ß√£o
  - **M√©dia de registros por semana** (m√©trica principal)

### Output
- **Relat√≥rio**: `relatorio_plataformas.csv`
  - Ordenado por m√©dia de registros/semana (plataformas mais ativas primeiro)
  - Formato Excel-BR (sep=';', decimal=',')

### Quando Usar
- Ap√≥s rodar o Notebook 1
- Para entender quais plataformas t√™m dados mais consistentes
- Para identificar plataformas com gaps ou sazonalidade
- **Pode rodar a qualquer momento**, n√£o bloqueia o fluxo principal

---

## NOTEBOOK 2: Valida√ß√£o e Filtragem

### Arquivo: `2_validacao_e_filtragem-Copy1.ipynb`

### Objetivo
Validar dados hist√≥ricos, identificar mudan√ßas semanais e filtrar apenas plataformas de interesse, consolidando em um √∫nico arquivo.

### Inputs
- **Parquets**: `dados_catalogo/staging/*.parquet`
- **Controle Manual**: `plataformas_controle.xlsx`
  - Colunas obrigat√≥rias: `Plataforma`, `USAR`
  - `USAR = 'S'` ‚Üí plataforma ser√° mantida
  - `USAR = 'N'` ‚Üí plataforma ser√° exclu√≠da

### Depend√™ncias
- Bibliotecas: pandas, pathlib, duckdb, openpyxl, datetime, IPython.display

### Processamento

#### Bloco 1: Leitura do Controle
- Carrega `plataformas_controle.xlsx`
- Valida presen√ßa das colunas obrigat√≥rias
- Cria lista `PLATAFORMAS_PARA_MANTER`

#### Bloco 2: Relat√≥rio de Valida√ß√£o
Gera an√°lise cronol√≥gica **semana a semana**:

**Para cada semana:**
1. **Novas Plataformas**: Aparecem pela primeira vez
2. **Plataformas que Retornaram**: Estavam ausentes e voltaram (gaps)
3. **Plataformas que Sa√≠ram**: Deixaram de reportar dados
4. **Varia√ß√µes de Cat√°logo**: Mudan√ßas >5% no total de registros

**Outputs do Bloco 2:**
- Tabela pivot completa (todas as plataformas x todas as semanas)
- `dados_catalogo/processed/relatorio_timeline_plataformas_TIMESTAMP.csv`
- `dados_catalogo/processed/relatorio_timeline_plataformas_TIMESTAMP.xlsx`
- Relat√≥rio visual no notebook (HTML formatado)

#### Bloco 3: Consolida√ß√£o e Filtragem Final
- **Otimiza√ß√£o**: Usa DuckDB para opera√ß√£o direta em SQL
  - Evita carregar DataFrame gigante em mem√≥ria
  - Aplica filtro de plataformas diretamente no banco
- Salva resultado consolidado em formato Parquet

### Output Principal
- **Arquivo**: `dados_catalogo/processed/dados_consolidados_filtrados.parquet`
  - Cont√©m **apenas** plataformas marcadas com `USAR = 'S'`
  - Uni√£o de todas as semanas (hist√≥rico completo)
  - Campo `data_ref` preservado para an√°lise temporal

### Caracter√≠sticas
- **Performance**: DuckDB para queries r√°pidas em grandes volumes
- **Controle Manual**: Usu√°rio decide quais plataformas manter
- **Rastreabilidade**: Relat√≥rios datados preservam hist√≥rico de an√°lises

---

## NOTEBOOK 3: Prepara√ß√£o da Tabela Dimens√£o

### Arquivo: `3_preparacao_splink1.ipynb`

### Objetivo
Criar uma tabela dimens√£o limpa e otimizada com **perfis √∫nicos por obra** (BB_UID), aplicando "crunch temporal" para eliminar duplicatas hist√≥ricas e preparar dados para processos de deduplica√ß√£o.

### Input
- **Arquivo**: `dados_catalogo/processed/dados_consolidados_filtrados.parquet`

### Depend√™ncias
- Bibliotecas: pandas, pathlib, duckdb, IPython.display, unidecode, textwrap

### Configura√ß√µes de Neg√≥cio

#### Semanas Problem√°ticas (hard-coded)
```python
SEMANAS_A_EXCLUIR = ['2024-10-25', '2024-11-21']
```
- Semanas identificadas com dados inconsistentes ou incompletos
- Exclu√≠das de toda an√°lise e crunch temporal

#### Colunas Selecionadas
```python
COLUNAS_PARA_SPLINK = [
    "BB_UID",              # Identificador √∫nico da obra
    "Type",                # Movie ou Series
    "BB_Title",            # T√≠tulo principal
    "BB_Year",             # Ano de lan√ßamento
    "BB_Duration",         # Dura√ß√£o em minutos
    "BB_Genres",           # G√™neros
    "BB_Directors",        # Diretores
    "BB_Original_Title",   # T√≠tulo original
    "IMDb_ID",            # Identificador IMDb
    "BB_Countries",        # Pa√≠ses de produ√ß√£o
    "BB_Primary_Country",  # Pa√≠s principal
    "BB_Production_Companies",
    "BB_Primary_Company",
    "TMDB_ID",            # Identificador TMDB
    "BB_Cast"             # Elenco
]
```

### Processamento

#### Bloco 1: An√°lise Explorat√≥ria de Completude
- Para cada coluna, calcula % de valores n√£o-nulos
- Visualiza√ß√£o com barras de progresso
- **Objetivo**: Entender quais campos s√£o mais confi√°veis

#### Bloco 2: An√°lise de Estabilidade
- Para cada BB_UID, verifica quantos valores distintos possui em cada campo ao longo das semanas
- % de UIDs com >1 valor = **instabilidade do campo**
- **Exemplo**: Se `BB_Title` muda para 20% dos UIDs ao longo do tempo, o campo √© inst√°vel
- Visualiza√ß√£o com gradiente de cor (verde = est√°vel, vermelho = inst√°vel)

#### Bloco 3: Crunch Temporal (Perfis √önicos)
Query SQL que:
1. Remove semanas da `SEMANAS_A_EXCLUIR`
2. Para cada `BB_UID`, mant√©m **apenas o registro mais recente** (`data_ref DESC`)
3. Seleciona apenas `COLUNAS_PARA_SPLINK`

**Resultado**: De ~4-5M registros ‚Üí ~225K perfis √∫nicos (obras distintas)

#### Bloco 4: Limpeza de Strings e Padroniza√ß√£o

**Fun√ß√µes Aplicadas:**
- `normalizar_string()`:
  - Converte para lowercase
  - Remove acentos (unidecode)
  - Remove espa√ßos extras
  
- `normalizar_lista_em_string()`:
  - Para campos como `BB_Cast`, `BB_Directors`
  - Split por v√≠rgula
  - Normaliza cada nome
  - Ordena alfabeticamente
  - Rejunta com v√≠rgula

**Campos Normalizados:**
- T√≠tulos: `BB_Title`, `BB_Original_Title`
- Listas: `BB_Cast`, `BB_Directors`
- Metadados: `BB_Primary_Country`

**Ajuste de Tipo:**
- `BB_Year`: convertido para `Int64` (aceita nulos)

#### Bloco 5 (ADICIONAL): Agrega√ß√£o de Plataformas
- Cria vers√£o enriquecida da tabela
- Para cada BB_UID, agrega **todas** as plataformas onde j√° esteve dispon√≠vel
- Join com dados hist√≥ricos completos

### Outputs

#### Output Principal
**Arquivo**: `dados_catalogo/processed/tabela_dimensao_limpa_para_splink.parquet`
- ~225K linhas (perfis √∫nicos por BB_UID)
- 15 colunas (conforme `COLUNAS_PARA_SPLINK`)
- Dados normalizados e limpos
- **Status de uso:** ‚ö†Ô∏è Preparado para Splink (notebooks 4-5 abandonados), n√£o usado na solu√ß√£o atual

#### Output Enriquecido
**Arquivo**: `dados_catalogo/processed/tabela_dimensao_com_plataformas.parquet`
- Mesmas 225K linhas
- 16 colunas (15 anteriores + `Plataformas`)
- Campo `Plataformas`: string com lista separada por v√≠rgula
- **Status de uso:** ‚ö†Ô∏è √ötil para an√°lise, mas n√£o integrado ao matching atual

### Caracter√≠sticas
- **Deduplica√ß√£o Temporal**: Elimina snapshots hist√≥ricos, mant√©m s√≥ perfil atual
- **Normaliza√ß√£o Agressiva**: Padroniza texto para facilitar compara√ß√µes
- **Preserva Hist√≥rico**: Vers√£o com plataformas mant√©m rastreabilidade
- **Memory-Efficient**: Usa DuckDB para opera√ß√µes pesadas

---

## M√©tricas de Redu√ß√£o

| Etapa | Registros | Descri√ß√£o |
|-------|-----------|-----------|
| **Staging** (ap√≥s Notebook 1) | ~4-5M | Todos os snapshots de todas as plataformas |
| **Filtrado** (ap√≥s Notebook 2) | ~2-3M | Apenas plataformas selecionadas |
| **Tabela Dimens√£o** (ap√≥s Notebook 3) | ~225K | Perfis √∫nicos (1 registro por BB_UID) |

**Redu√ß√£o**: ~95% (4-5M ‚Üí 225K)

**‚ö†Ô∏è Status:** Dados preparados, mas n√£o integrados ao fluxo de matching atual. A solu√ß√£o em produ√ß√£o usa o XLSX mais recente (~450K registros sem redu√ß√£o temporal).

---

## Arquivos de Configura√ß√£o

### mapeamento_colunas.json
```json
{
  "BB_UID": "str",
  "Platform_Name": "str",
  "BB_Title": "str",
  "BB_Year": "float",
  ...
}
```
- Define schema de importa√ß√£o
- Evita carregar colunas desnecess√°rias
- Inclui tipo de dado esperado

### plataformas_controle.xlsx
| Plataforma | USAR |
|------------|------|
| Netflix | S |
| Disney+ | S |
| Pluto TV | N |
| ... | ... |

- Controle manual de quais plataformas processar
- Coluna `USAR`: valores aceitos = `'S'` ou `'N'` (case insensitive)

---

## Troubleshooting

### "Colunas n√£o mapeadas" no Notebook 1
- **Causa**: Novo campo apareceu nos arquivos Excel
- **Solu√ß√£o**: Adicionar campo ao `mapeamento_colunas.json` se relevante

### "Plataforma n√£o encontrada" no Notebook 2
- **Causa**: Nova plataforma nos dados, n√£o est√° em `plataformas_controle.xlsx`
- **Solu√ß√£o**: 
  1. Consultar relat√≥rio de valida√ß√£o (se√ß√£o "Novas Plataformas")
  2. Adicionar √† planilha com flag `'S'` ou `'N'`
  3. Reprocessar Notebook 2

### Mem√≥ria insuficiente
- **Notebooks 1 e 2**: Usar DuckDB (j√° implementado)
- **Notebook 3**: 
  - Reduzir `COLUNAS_PARA_SPLINK` (menos campos)
  - Processar em lotes por plataforma (n√£o implementado)

### Valores estranhos na an√°lise de estabilidade
- **Valores altos** (>30%): Campo naturalmente vari√°vel (ex: `BB_Score`, `Package`)
- **Valores m√©dios** (10-30%): Inconsist√™ncias nas fontes de dados
- **Valores baixos** (<10%): Campos confi√°veis (ex: `BB_Year`, `IMDb_ID`)

---

## Depend√™ncias do Ambiente

```bash
# Principais bibliotecas
pandas>=2.0
openpyxl>=3.0
pyarrow>=10.0
duckdb>=0.9
unidecode>=1.3
tqdm>=4.60

# Jupyter
ipykernel
IPython
```

---

## Notas Importantes

1. **Ordem de Execu√ß√£o**: Notebooks devem ser rodados na sequ√™ncia (1 ‚Üí 2 ‚Üí 3)
2. **Notebook 1.1**: Opcional, pode rodar ap√≥s Notebook 1 sem afetar o fluxo
3. **Notebook 3.1 (Splink_Exploracao)**: ‚ùå **DESCARTADO** - l√™ arquivo errado (`dados_consolidados_filtrados.parquet` em vez de `tabela_dimensao_limpa_para_splink.parquet`), causando duplicatas temporais
4. **Incremental**: Notebook 1 usa manifesto para processar apenas arquivos novos
5. **Controle Manual**: Notebook 2 exige atualiza√ß√£o manual de `plataformas_controle.xlsx`
6. **Semanas Exclu√≠das**: Valores hard-coded no Notebook 3 - ajustar conforme necess√°rio
   ```python
   SEMANAS_A_EXCLUIR = ['2024-10-25', '2024-11-21']
   ```
   - Documentar o motivo da exclus√£o (dados corrompidos/incompletos)

---

## Estado Atual do Projeto

### ‚úÖ O que est√° funcionando:
- **Notebooks 1-3**: Prepara√ß√£o hist√≥rica completa e validada
- **Matching determin√≠stico**: Implementado e documentado em `pipeline_cpb_uid.md`
- **Base t0**: Consolidada e enriquecida (ano passado)

### ‚ö†Ô∏è O que est√° em standby:
- **Hist√≥rico em Parquet**: Preparado mas n√£o integrado ao matching
- **Notebooks 4-5 (Splink)**: Abandonados

### üî® O que falta implementar:
- **Crunch temporal**: Gerar IN/OUT com toler√¢ncia de 22 dias
- **Integra√ß√£o t0 + hist√≥rico**: Unificar bases de anos diferentes
- **Expans√£o do XLSX atual**: Adicionar obras que sa√≠ram + colunas temporais
- **Pipeline incremental**: Processar apenas novos snapshots semanais

**Nota para retorno futuro:** Ao voltar a este projeto em alguns meses, comece pelo `pipeline_cpb_uid.md` para entender o matching em produ√ß√£o. Os notebooks 1-3 s√£o prepara√ß√£o de dados hist√≥ricos que aguardam integra√ß√£o.

---

## Contexto: Tentativas com Splink (Notebooks 4-5)

**Status:** ‚ùå Abandonados

Ap√≥s os notebooks 1-3, foram desenvolvidos:
- **Notebook 4**: Tentativa de deduplica√ß√£o interna usando Splink
- **Notebook 5**: Tentativa de matching com base Ancine usando Splink

**Por que foram abandonados:**
- Performance insatisfat√≥ria em grandes volumes
- Dificuldade de calibra√ß√£o dos modelos probabil√≠sticos
- Necessidade de solu√ß√£o mais determin√≠stica e control√°vel

**Solu√ß√£o atual implementada:**
- Matching determin√≠stico baseado em regras (T√≠tulo + Ano + Diretor/Produtora)
- Documentado em: `pipeline_cpb_uid.md`
- **Trabalha apenas com o snapshot XLSX mais recente** (n√£o usa o hist√≥rico em Parquet)
- Redu√ß√£o de escopo: foco no cat√°logo atual (mais importante para o neg√≥cio)

---

## Lacunas a Implementar

Os notebooks 1-3 preparam os dados hist√≥ricos, mas **n√£o est√£o sendo usados** na solu√ß√£o de matching atual. Para integr√°-los ao fluxo final, ser√° necess√°rio:

### 1. Crunch Temporal com IN/OUT
**Objetivo:** A partir dos Parquets hist√≥ricos, criar tabela com:
- `BB_UID`: Identificador √∫nico da obra
- `First_Seen`: Primeira data em que a obra apareceu (IN)
- `Last_Seen`: √öltima data em que a obra apareceu (OUT)
- **Regra de toler√¢ncia**: Gap de at√© 22 dias n√£o caracteriza sa√≠da de cat√°logo
  - Motivo: Falhas no scraping podem causar aus√™ncias tempor√°rias
  - Obras raramente ficam apenas 1 semana fora do cat√°logo

**Input:** `dados_catalogo/staging/*.parquet` (todos os snapshots semanais)

**Output:** Tabela temporal ainda n√£o implementada

### 2. Integra√ß√£o com Base t0
**Objetivo:** Juntar base hist√≥rica (ano passado) com dados atuais

**Desafios conhecidos:**
- Esquemas diferentes entre t0 e dados atuais
- Campos com nomes/formatos distintos
- Necessidade de mapeamento entre as bases

### 3. Expans√£o do XLSX Mais Recente
**Objetivo:** Enriquecer snapshot atual com hist√≥rico temporal

**Opera√ß√µes necess√°rias:**
- Adicionar obras que sa√≠ram de cat√°logo (presentes no hist√≥rico, ausentes no atual)
- Imputar colunas IN/OUT em todas as obras
- Preparar para aplica√ß√£o do matching determin√≠stico (pipeline_cpb_uid.md)

---

## Pr√≥ximos Passos (Roadmap)

### Curto Prazo (Constru√ß√£o da Base t1)
1. ‚úÖ Preparar hist√≥rico semanal (Notebooks 1-3) - **CONCLU√çDO**
2. ‚è≥ Implementar crunch temporal com IN/OUT (toler√¢ncia 22 dias) - **PENDENTE**
3. ‚è≥ Integrar com base t0 - **PENDENTE**
4. ‚è≥ Expandir XLSX mais recente com temporal - **PENDENTE**
5. ‚úÖ Aplicar matching determin√≠stico - **CONCLU√çDO** (pipeline_cpb_uid.md)

### M√©dio Prazo (Ap√≥s Base t1 Consolidada)
- Adaptar pipeline para processamento incremental (semanal ou quinzenal)
- Automatizar detec√ß√£o de IN/OUT para novas obras
- Reduzir depend√™ncia de processamento hist√≥rico completo

---
