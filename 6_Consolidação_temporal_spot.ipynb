{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1073262b-439f-4809-996d-f0718e06978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTEBOOK 6: INTEGRA√á√ÉO TEMPORAL - BASE t0 + PARQUETS t1\n",
    "# ============================================================================\n",
    "# Objetivo: Consolidar base hist√≥rica (t0) com snapshots semanais (Parquets)\n",
    "#           gerando colunas First Detected Date e Last Detected Date\n",
    "#\n",
    "# Inputs:\n",
    "#   - Base_t0.xlsx (base consolidada at√© agosto/2024)\n",
    "#   - dados_catalogo/staging/*.parquet (julho/2024 ‚Üí agosto/2025)\n",
    "#\n",
    "# Output:\n",
    "#   - Base_Consolidada_t0_t1_YYYY-MM-DD.xlsx\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2504bd-04af-4b04-9a84-dc7ad85aa487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bibliotecas importadas com sucesso\n",
      "üïí In√≠cio do processamento: 2025-10-13 17:56:56\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 1: Imports e Configura√ß√£o\n",
    "# ----------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso\")\n",
    "print(f\"üïí In√≠cio do processamento: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee676c3-fde6-4ffd-9e69-c103c8695205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Constantes e mapeamentos definidos\n",
      "   üìÇ Base t0: Base tratada 2024.xlsx\n",
      "   üìÇ Parquets: dados_catalogo\\staging\\*.parquet\n",
      "   üìÖ Data de corte: 30/08/2024\n",
      "   ‚è±Ô∏è  Gap toler√¢ncia: 22 dias\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 2: Defini√ß√£o de Constantes e Mapeamentos\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# === CAMINHOS ===\n",
    "ARQUIVO_T0 = pathlib.Path('Base tratada 2024.xlsx')  \n",
    "PASTA_PARQUETS = pathlib.Path('dados_catalogo/staging')\n",
    "CAMINHO_PARQUETS = str(PASTA_PARQUETS / '*.parquet')\n",
    "PASTA_OUTPUT = pathlib.Path('dados_catalogo/processed')\n",
    "PASTA_OUTPUT.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# === CONSTANTES ===\n",
    "DATA_CORTE = '30/08/2024'  # Data de refer√™ncia para fechamento\n",
    "GAP_TOLERANCIA_DIAS = 22   # Toler√¢ncia para considerar per√≠odos distintos\n",
    "DATA_INICIO_OVERLAP = '11/07/2024'  # In√≠cio do per√≠odo de overlap\n",
    "\n",
    "# === MAPEAMENTO DE PACKAGE ===\n",
    "MAPEAMENTO_PACKAGE = {\n",
    "    'TVOD': 'Transaction VOD',\n",
    "    'SVOD': 'Subscription VOD',\n",
    "    'Validated VOD': 'Validated VOD',\n",
    "    'TV Everywhere': 'TV Everywhere',\n",
    "    'Free VOD': 'Advertising VOD/Free VOD'\n",
    "    }\n",
    "\n",
    "# Criar dicion√°rio reverso para normaliza√ß√£o\n",
    "PACKAGE_NORMALIZADO = {\n",
    "    # === SVOD ===\n",
    "    'svod': 'SVOD',\n",
    "    'subscription': 'SVOD',\n",
    "    'subscription vod': 'SVOD',\n",
    "    \n",
    "    # === TVOD ===\n",
    "    'tvod': 'TVOD',\n",
    "    'transactional': 'TVOD',\n",
    "    'transaction vod': 'TVOD',\n",
    "    \n",
    "    # === Free VOD ===\n",
    "    'free vod': 'Free VOD',\n",
    "    'free': 'Free VOD',\n",
    "    'free/free with ads': 'Free VOD',\n",
    "    'advertising vod/free vod': 'Free VOD',\n",
    "    \n",
    "    # === TV Everywhere ===\n",
    "    'tv everywhere': 'TV Everywhere',\n",
    "    \n",
    "    # === Validated VOD ===\n",
    "    'validated vod': 'Validated VOD'\n",
    "}\n",
    "\n",
    "for xlsx_val, parquet_val in MAPEAMENTO_PACKAGE.items():\n",
    "    PACKAGE_NORMALIZADO[xlsx_val.lower().strip()] = xlsx_val\n",
    "    PACKAGE_NORMALIZADO[parquet_val.lower().strip()] = xlsx_val\n",
    "\n",
    "print(\"‚úÖ Constantes e mapeamentos definidos\")\n",
    "print(f\"   üìÇ Base t0: {ARQUIVO_T0}\")\n",
    "print(f\"   üìÇ Parquets: {CAMINHO_PARQUETS}\")\n",
    "print(f\"   üìÖ Data de corte: {DATA_CORTE}\")\n",
    "print(f\"   ‚è±Ô∏è  Gap toler√¢ncia: {GAP_TOLERANCIA_DIAS} dias\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a378030d-bf9f-485b-b617-2771dcbb2f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üì• CARREGANDO BASE t0\n",
      "======================================================================\n",
      "‚úÖ Base t0 carregada: 383,288 linhas x 28 colunas\n",
      "‚úÖ Todas as colunas essenciais presentes\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 3: Carregamento da Base t0\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì• CARREGANDO BASE t0\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Carregar base t0\n",
    "df_t0 = pd.read_excel(ARQUIVO_T0)\n",
    "\n",
    "print(f\"‚úÖ Base t0 carregada: {len(df_t0):,} linhas x {len(df_t0.columns)} colunas\")\n",
    "\n",
    "# Renomear colunas para padr√£o dos Parquets\n",
    "mapeamento_colunas_t0 = {\n",
    "    'Platform': 'Platform_Name',\n",
    "    'Plan/Channel': 'Channel',\n",
    "    'First Detected Date': 'First_Detected',\n",
    "    'Last Detected Date': 'Last_Detected'\n",
    "}\n",
    "\n",
    "df_t0 = df_t0.rename(columns=mapeamento_colunas_t0)\n",
    "\n",
    "# Verificar colunas essenciais\n",
    "colunas_essenciais = ['UID', 'Platform_Name', 'Package', 'Channel', 'First_Detected', 'Last_Detected']\n",
    "colunas_faltando = [col for col in colunas_essenciais if col not in df_t0.columns]\n",
    "\n",
    "if colunas_faltando:\n",
    "    print(f\"‚ö†Ô∏è  ATEN√á√ÉO: Colunas faltando na t0: {colunas_faltando}\")\n",
    "    print(\"   Verifique os nomes das colunas e ajuste o mapeamento acima.\")\n",
    "else:\n",
    "    print(\"‚úÖ Todas as colunas essenciais presentes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7303d96c-0635-40d4-a320-4774bda8a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chaves √∫nicas criadas: 329,122 combina√ß√µes distintas\n",
      "‚úÖ Datas convertidas para datetime\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 3.1: Criar chaves\n",
    "# ----------------------------------------------------------------------------\n",
    "# Criar chave √∫nica\n",
    "df_t0['chave'] = (\n",
    "    df_t0['UID'].astype(str) + '|' + \n",
    "    df_t0['Platform_Name'].astype(str) + '|' + \n",
    "    df_t0['Package'].fillna('').astype(str) + '|' + \n",
    "    df_t0['Channel'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Chaves √∫nicas criadas: {df_t0['chave'].nunique():,} combina√ß√µes distintas\")\n",
    "\n",
    "# Converter datas para datetime (formato dd/mm/yyyy)\n",
    "df_t0['First_Detected'] = pd.to_datetime(df_t0['First_Detected'], format='%d/%m/%Y', errors='coerce')\n",
    "df_t0['Last_Detected'] = pd.to_datetime(df_t0['Last_Detected'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "print(f\"‚úÖ Datas convertidas para datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb823e2-0b93-4801-9cf7-4a472f5620c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üì• CARREGANDO PARQUETS COM DUCKDB\n",
      "======================================================================\n",
      "üîÑ Executando query DuckDB...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b166bf36b8e4e51a9bbd265179ca252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parquets carregados: 20,985,955 registros\n",
      "   üìÖ Per√≠odo: 2024-07-11 ‚Üí 2025-08-28\n",
      "   üéØ UIDs √∫nicos: 301,413\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 4: Carregamento e Prepara√ß√£o dos Parquets (DuckDB)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì• CARREGANDO PARQUETS COM DUCKDB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# Query para carregar todos os Parquets\n",
    "query_load_parquets = f\"\"\"\n",
    "SELECT \n",
    "    \"BB_UID\" as UID,\n",
    "    \"Platform_Name\",\n",
    "    \"Package\",\n",
    "    \"Channel\",\n",
    "    \"data_ref\"\n",
    "FROM read_parquet('{CAMINHO_PARQUETS}', union_by_name=true)\n",
    "WHERE \"BB_UID\" IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Executando query DuckDB...\")\n",
    "df_parquets_raw = con.execute(query_load_parquets).fetch_df()\n",
    "\n",
    "print(f\"‚úÖ Parquets carregados: {len(df_parquets_raw):,} registros\")\n",
    "print(f\"   üìÖ Per√≠odo: {df_parquets_raw['data_ref'].min()} ‚Üí {df_parquets_raw['data_ref'].max()}\")\n",
    "print(f\"   üéØ UIDs √∫nicos: {df_parquets_raw['UID'].nunique():,}\")\n",
    "\n",
    "# Converter data_ref para datetime\n",
    "df_parquets_raw['data_ref'] = pd.to_datetime(df_parquets_raw['data_ref'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079e6f80-17a5-4d38-a5fa-c6b32606d5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß NORMALIZANDO CAMPO PACKAGE\n",
      "======================================================================\n",
      "‚úÖ Package normalizado\n",
      "\n",
      "üìä Distribui√ß√£o de Package nos Parquets (top 10):\n",
      "Package_normalizado\n",
      "SVOD             7396673\n",
      "Free VOD         7236428\n",
      "TVOD             3448681\n",
      "TV Everywhere    2700449\n",
      "Validated VOD     203724\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 5: Normaliza√ß√£o de Package\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîß NORMALIZANDO CAMPO PACKAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def normalizar_package(package_value):\n",
    "    \"\"\"Normaliza valores de Package usando o mapeamento definido\"\"\"\n",
    "    if pd.isna(package_value):\n",
    "        return ''\n",
    "    \n",
    "    valor_limpo = str(package_value).lower().strip()\n",
    "    return PACKAGE_NORMALIZADO.get(valor_limpo, str(package_value))\n",
    "\n",
    "# Aplicar normaliza√ß√£o\n",
    "df_t0['Package_normalizado'] = df_t0['Package'].apply(normalizar_package)\n",
    "df_parquets_raw['Package_normalizado'] = df_parquets_raw['Package'].apply(normalizar_package)\n",
    "\n",
    "print(\"‚úÖ Package normalizado\")\n",
    "\n",
    "# Recriar chave com Package normalizado\n",
    "df_t0['chave'] = (\n",
    "    df_t0['UID'].astype(str) + '|' + \n",
    "    df_t0['Platform_Name'].astype(str) + '|' + \n",
    "    df_t0['Package_normalizado'].fillna('').astype(str) + '|' + \n",
    "    df_t0['Channel'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "df_parquets_raw['chave'] = (\n",
    "    df_parquets_raw['UID'].astype(str) + '|' + \n",
    "    df_parquets_raw['Platform_Name'].astype(str) + '|' + \n",
    "    df_parquets_raw['Package_normalizado'].fillna('').astype(str) + '|' + \n",
    "    df_parquets_raw['Channel'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "# Verificar distribui√ß√£o de Package nos Parquets\n",
    "print(\"\\nüìä Distribui√ß√£o de Package nos Parquets (top 10):\")\n",
    "print(df_parquets_raw['Package_normalizado'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3202611-053b-446f-96a0-a40a66e53dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√£o de detec√ß√£o de gaps definida\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 6: Fun√ß√£o de Detec√ß√£o de Gaps (22 dias)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def detectar_periodos_com_gap(datas_serie, tolerancia_dias=22):\n",
    "    \"\"\"\n",
    "    Detecta per√≠odos distintos em uma s√©rie de datas, considerando gap de toler√¢ncia.\n",
    "    \n",
    "    Args:\n",
    "        datas_serie: pandas Series com datas\n",
    "        tolerancia_dias: gap em dias para considerar novo per√≠odo\n",
    "        \n",
    "    Returns:\n",
    "        lista de dicion√°rios com {'inicio': data, 'fim': data}\n",
    "    \"\"\"\n",
    "    if len(datas_serie) == 0:\n",
    "        return []\n",
    "    \n",
    "    datas_ordenadas = sorted(datas_serie.dropna().unique())\n",
    "    \n",
    "    if len(datas_ordenadas) == 0:\n",
    "        return []\n",
    "    \n",
    "    periodos = []\n",
    "    periodo_atual = {'inicio': datas_ordenadas[0], 'fim': datas_ordenadas[0]}\n",
    "    \n",
    "    for i in range(1, len(datas_ordenadas)):\n",
    "        data_anterior = datas_ordenadas[i-1]\n",
    "        data_atual = datas_ordenadas[i]\n",
    "        \n",
    "        gap_dias = (data_atual - data_anterior).days\n",
    "        \n",
    "        if gap_dias <= tolerancia_dias:\n",
    "            # Continua no mesmo per√≠odo\n",
    "            periodo_atual['fim'] = data_atual\n",
    "        else:\n",
    "            # Gap excedeu toler√¢ncia - fecha per√≠odo atual e abre novo\n",
    "            periodos.append(periodo_atual.copy())\n",
    "            periodo_atual = {'inicio': data_atual, 'fim': data_atual}\n",
    "    \n",
    "    # Adicionar √∫ltimo per√≠odo\n",
    "    periodos.append(periodo_atual)\n",
    "    \n",
    "    return periodos\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o de detec√ß√£o de gaps definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61d2ad42-ba96-4721-8175-68198648f0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîÑ PROCESSANDO GRUPO 1: OBRAS DA BASE t0 (OTIMIZADO)\n",
      "======================================================================\n",
      "üìä Total de linhas na t0: 383,288\n",
      "üöÄ Processando com DuckDB (vetorizado)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ce72d33d2746a4b5b32900411d6442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Obras da t0 com apari√ß√µes nos Parquets: 310,149 per√≠odos\n",
      "‚úÖ Obras da t0 SEM apari√ß√µes nos Parquets: 166,499 linhas\n",
      "‚úÖ Grupo 1 TOTAL: 476,648 linhas\n",
      "   üìä Chaves √∫nicas: 329,122\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 7: Processamento Grupo 1 - Obras da t0 (OTIMIZADO COM DUCKDB)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÑ PROCESSANDO GRUPO 1: OBRAS DA BASE t0 (OTIMIZADO)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Registrar t0 no DuckDB\n",
    "con.register('df_t0_reg', df_t0)\n",
    "con.register('df_parquets_reg', df_parquets_raw)\n",
    "\n",
    "print(f\"üìä Total de linhas na t0: {len(df_t0):,}\")\n",
    "print(\"üöÄ Processando com DuckDB (vetorizado)...\")\n",
    "\n",
    "# Query SQL para processar Grupo 1\n",
    "query_grupo1 = \"\"\"\n",
    "WITH \n",
    "-- 1. Apari√ß√µes nos Parquets para chaves da t0\n",
    "aparicoes_t0 AS (\n",
    "    SELECT \n",
    "        p.chave,\n",
    "        p.data_ref,\n",
    "        t.First_Detected as first_t0,\n",
    "        t.Last_Detected as last_t0\n",
    "    FROM df_parquets_reg p\n",
    "    INNER JOIN df_t0_reg t ON p.chave = t.chave\n",
    "),\n",
    "\n",
    "-- 2. Detectar gaps usando window function\n",
    "gaps_detectados AS (\n",
    "    SELECT \n",
    "        chave,\n",
    "        data_ref,\n",
    "        first_t0,\n",
    "        last_t0,\n",
    "        LAG(data_ref) OVER (PARTITION BY chave ORDER BY data_ref) as data_anterior,\n",
    "        DATEDIFF('day', \n",
    "            LAG(data_ref) OVER (PARTITION BY chave ORDER BY data_ref), \n",
    "            data_ref\n",
    "        ) as gap_dias\n",
    "    FROM aparicoes_t0\n",
    "),\n",
    "\n",
    "-- 3. Atribuir per√≠odo baseado em gaps > 22 dias\n",
    "periodos_numerados AS (\n",
    "    SELECT \n",
    "        chave,\n",
    "        data_ref,\n",
    "        first_t0,\n",
    "        last_t0,\n",
    "        SUM(CASE WHEN gap_dias > 22 OR gap_dias IS NULL THEN 1 ELSE 0 END) \n",
    "            OVER (PARTITION BY chave ORDER BY data_ref \n",
    "                  ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as periodo_id\n",
    "    FROM gaps_detectados\n",
    "),\n",
    "\n",
    "-- 4. Agregar por chave + per√≠odo\n",
    "periodos_agregados AS (\n",
    "    SELECT \n",
    "        chave,\n",
    "        periodo_id,\n",
    "        MIN(first_t0) as first_t0,\n",
    "        MAX(last_t0) as last_t0,\n",
    "        MIN(data_ref) as first_parquet,\n",
    "        MAX(data_ref) as last_parquet,\n",
    "        COUNT(*) as num_aparicoes\n",
    "    FROM periodos_numerados\n",
    "    GROUP BY chave, periodo_id\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    chave,\n",
    "    periodo_id,\n",
    "    first_t0,\n",
    "    last_t0,\n",
    "    first_parquet,\n",
    "    last_parquet,\n",
    "    num_aparicoes,\n",
    "    -- Decidir First final: usar t0 se for anterior aos Parquets\n",
    "    CASE \n",
    "        WHEN first_t0 < first_parquet THEN first_t0\n",
    "        ELSE first_parquet\n",
    "    END as First_Detected_final,\n",
    "    -- Decidir Last final: NULL se √∫ltimo per√≠odo est√° recente\n",
    "    CASE \n",
    "        WHEN last_parquet >= '2025-07-01' THEN NULL\n",
    "        ELSE last_parquet\n",
    "    END as Last_Detected_final\n",
    "FROM periodos_agregados\n",
    "\"\"\"\n",
    "\n",
    "df_grupo1_parquets = con.execute(query_grupo1).fetch_df()\n",
    "\n",
    "print(f\"‚úÖ Obras da t0 com apari√ß√µes nos Parquets: {len(df_grupo1_parquets):,} per√≠odos\")\n",
    "\n",
    "# Agora buscar obras da t0 que N√ÉO aparecem nos Parquets (mant√™m dados originais)\n",
    "query_t0_sem_parquets = \"\"\"\n",
    "SELECT DISTINCT t.*\n",
    "FROM df_t0_reg t\n",
    "LEFT JOIN df_parquets_reg p ON t.chave = p.chave\n",
    "WHERE p.chave IS NULL\n",
    "\"\"\"\n",
    "\n",
    "df_grupo1_sem_parquets = con.execute(query_t0_sem_parquets).fetch_df()\n",
    "print(f\"‚úÖ Obras da t0 SEM apari√ß√µes nos Parquets: {len(df_grupo1_sem_parquets):,} linhas\")\n",
    "\n",
    "# Para as que t√™m apari√ß√µes, fazer join para trazer metadados completos\n",
    "df_grupo1_com_metadados = df_grupo1_parquets.merge(\n",
    "    df_t0[['chave', 'UID', 'Platform_Name', 'Package', 'Channel']].drop_duplicates(),\n",
    "    on='chave',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Renomear colunas finais\n",
    "df_grupo1_com_metadados = df_grupo1_com_metadados.rename(columns={\n",
    "    'First_Detected_final': 'First_Detected',\n",
    "    'Last_Detected_final': 'Last_Detected'\n",
    "})\n",
    "\n",
    "# Consolidar Grupo 1\n",
    "df_grupo1 = pd.concat([\n",
    "    df_grupo1_com_metadados[['UID', 'Platform_Name', 'Package', 'Channel', 'First_Detected', 'Last_Detected', 'chave']],\n",
    "    df_grupo1_sem_parquets[['UID', 'Platform_Name', 'Package', 'Channel', 'First_Detected', 'Last_Detected', 'chave']]\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Grupo 1 TOTAL: {len(df_grupo1):,} linhas\")\n",
    "print(f\"   üìä Chaves √∫nicas: {df_grupo1['chave'].nunique():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c09138bc-0bb6-4c69-b1b8-c791e767c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç DIAGN√ìSTICO: INVESTIGANDO GRUPO 1\n",
      "======================================================================\n",
      "\n",
      "üìä Chaves que AUMENTARAM linhas: 103,632\n",
      "üìä Total de linhas extras criadas: 107,785\n",
      "\n",
      "üî¥ Top 10 chaves com mais linhas extras:\n",
      "\n",
      "1. Chave: 2314393f9cf6b09a44b063f5a8d15e14|Claro TV+|TVOD|...\n",
      "   t0: 1 linhas ‚Üí Grupo1: 5 linhas (+ 4)\n",
      "   Per√≠odos na t0:\n",
      "      01/10/2022 ‚Üí 26/07/2024\n",
      "   Per√≠odos no Grupo1:\n",
      "      2022-10-01 00:00:00 ‚Üí NaT\n",
      "      2022-10-01 00:00:00 ‚Üí 2024-11-01 00:00:00\n",
      "      2022-10-01 00:00:00 ‚Üí 2024-07-19 00:00:00\n",
      "      2022-10-01 00:00:00 ‚Üí 2024-09-13 00:00:00\n",
      "      2022-10-01 00:00:00 ‚Üí NaT\n",
      "\n",
      "2. Chave: 2314393f9cf6b09a44b063f5a8d15e14|Claro TV+|TV Everywhere|...\n",
      "   t0: 1 linhas ‚Üí Grupo1: 5 linhas (+ 4)\n",
      "   Per√≠odos na t0:\n",
      "      01/10/2022 ‚Üí 26/07/2024\n",
      "   Per√≠odos no Grupo1:\n",
      "      2022-10-01 00:00:00 ‚Üí 2024-07-19 00:00:00\n",
      "      2022-10-01 00:00:00 ‚Üí 2024-11-01 00:00:00\n",
      "      2022-10-01 00:00:00 ‚Üí NaT\n",
      "      2022-10-01 00:00:00 ‚Üí NaT\n",
      "      2022-10-01 00:00:00 ‚Üí 2024-09-13 00:00:00\n",
      "\n",
      "3. Chave: 693263f571b5fb6172ca58a1749daa6d|Hoichoi|Free VOD|...\n",
      "   t0: 1 linhas ‚Üí Grupo1: 5 linhas (+ 4)\n",
      "   Per√≠odos na t0:\n",
      "      12/07/2022 ‚Üí vazio\n",
      "   Per√≠odos no Grupo1:\n",
      "      2022-07-12 00:00:00 ‚Üí NaT\n",
      "      2022-07-12 00:00:00 ‚Üí 2024-09-27 00:00:00\n",
      "      2022-07-12 00:00:00 ‚Üí 2024-08-15 00:00:00\n",
      "      2022-07-12 00:00:00 ‚Üí 2024-12-06 00:00:00\n",
      "      2022-07-12 00:00:00 ‚Üí NaT\n",
      "\n",
      "4. Chave: 4d71e94409c1c9e97cb86c38e6ed3627|Plex|Free VOD|...\n",
      "   t0: 1 linhas ‚Üí Grupo1: 5 linhas (+ 4)\n",
      "   Per√≠odos na t0:\n",
      "      01/08/2024 ‚Üí vazio\n",
      "   Per√≠odos no Grupo1:\n",
      "      2024-08-01 00:00:00 ‚Üí NaT\n",
      "      2024-08-01 00:00:00 ‚Üí 2024-10-19 00:00:00\n",
      "      2024-08-01 00:00:00 ‚Üí 2024-08-02 00:00:00\n",
      "      2024-08-01 00:00:00 ‚Üí 2024-12-06 00:00:00\n",
      "      2024-08-01 00:00:00 ‚Üí NaT\n",
      "\n",
      "5. Chave: bb39fdd05c0de8e11252320ca4ea4c88|Claro TV+|TV Everywhere|...\n",
      "   t0: 1 linhas ‚Üí Grupo1: 5 linhas (+ 4)\n",
      "   Per√≠odos na t0:\n",
      "      24/01/2022 ‚Üí vazio\n",
      "   Per√≠odos no Grupo1:\n",
      "      2022-01-24 00:00:00 ‚Üí NaT\n",
      "      2022-01-24 00:00:00 ‚Üí 2024-08-15 00:00:00\n",
      "      2022-01-24 00:00:00 ‚Üí 2024-07-11 00:00:00\n",
      "      2022-01-24 00:00:00 ‚Üí 2024-12-06 00:00:00\n",
      "      2022-01-24 00:00:00 ‚Üí 2024-09-13 00:00:00\n",
      "\n",
      "6. Chave: cb6735062a64f143a74767462454e937|Zee5|Free VOD|...\n",
      "   t0: 1 linhas ‚Üí Grupo1: 5 linhas (+ 4)\n",
      "   Per√≠odos na t0:\n",
      "      27/10/2022 ‚Üí 09/08/2024\n",
      "   Per√≠odos no Grupo1:\n",
      "      2022-10-27 00:00:00 ‚Üí 2024-09-13 00:00:00\n",
      "      2022-10-27 00:00:00 ‚Üí 2024-12-13 00:00:00\n",
      "      2022-10-27 00:00:00 ‚Üí 2024-07-11 00:00:00\n",
      "      2022-10-27 00:00:00 ‚Üí 2024-08-08 00:00:00\n",
      "      2022-10-27 00:00:00 ‚Üí NaT\n",
      "\n",
      "7. Chave: f4e4182350493eb79e29fc56b932a1ad|UOL Play|SVOD|...\n",
      "   t0: 1 linhas ‚Üí Grupo1: 5 linhas (+ 4)\n",
      "   Per√≠odos na t0:\n",
      "      24/03/2021 ‚Üí vazio\n",
      "   Per√≠odos no Grupo1:\n",
      "      2021-03-24 00:00:00 ‚Üí 2024-10-04 00:00:00\n",
      "      2021-03-24 00:00:00 ‚Üí NaT\n",
      "      2021-03-24 00:00:00 ‚Üí 2024-12-13 00:00:00\n",
      "      2021-03-24 00:00:00 ‚Üí 2024-11-08 00:00:00\n",
      "      2021-03-24 00:00:00 ‚Üí 2024-08-23 00:00:00\n",
      "\n",
      "8. Chave: f653a9e98faddc65643ac9be6349005a|Claro TV+|TV Everywhere|...\n",
      "   t0: 1 linhas ‚Üí Grupo1: 5 linhas (+ 4)\n",
      "   Per√≠odos na t0:\n",
      "      25/05/2020 ‚Üí vazio\n",
      "   Per√≠odos no Grupo1:\n",
      "      2020-05-25 00:00:00 ‚Üí 2024-12-06 00:00:00\n",
      "      2020-05-25 00:00:00 ‚Üí 2024-11-08 00:00:00\n",
      "      2020-05-25 00:00:00 ‚Üí 2024-07-11 00:00:00\n",
      "      2020-05-25 00:00:00 ‚Üí 2024-08-15 00:00:00\n",
      "      2020-05-25 00:00:00 ‚Üí NaT\n",
      "\n",
      "9. Chave: 4ae1650347c61a75ecf2cef62deba24a|Claro TV+|TVOD|...\n",
      "   t0: 1 linhas ‚Üí Grupo1: 4 linhas (+ 3)\n",
      "   Per√≠odos na t0:\n",
      "      01/07/2021 ‚Üí vazio\n",
      "   Per√≠odos no Grupo1:\n",
      "      2021-07-01 00:00:00 ‚Üí 2024-09-07 00:00:00\n",
      "      2021-07-01 00:00:00 ‚Üí 2024-11-21 00:00:00\n",
      "      2021-07-01 00:00:00 ‚Üí NaT\n",
      "      2021-07-01 00:00:00 ‚Üí 2024-07-11 00:00:00\n",
      "\n",
      "10. Chave: 178dab6375df7fe25905e242e0ad6c77|Sky+|SVOD|...\n",
      "   t0: 1 linhas ‚Üí Grupo1: 4 linhas (+ 3)\n",
      "   Per√≠odos na t0:\n",
      "      18/04/2022 ‚Üí vazio\n",
      "   Per√≠odos no Grupo1:\n",
      "      2022-04-18 00:00:00 ‚Üí 2024-11-08 00:00:00\n",
      "      2022-04-18 00:00:00 ‚Üí NaT\n",
      "      2022-04-18 00:00:00 ‚Üí 2024-12-13 00:00:00\n",
      "      2022-04-18 00:00:00 ‚Üí 2024-09-13 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# DIAGN√ìSTICO GRUPO 1: Por que 476K linhas?\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç DIAGN√ìSTICO: INVESTIGANDO GRUPO 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Criar chave temp\n",
    "df_grupo1['chave_temp'] = (\n",
    "    df_grupo1['UID'].astype(str) + '|' + \n",
    "    df_grupo1['Platform_Name'].astype(str) + '|' + \n",
    "    df_grupo1['Package'].fillna('').astype(str) + '|' + \n",
    "    df_grupo1['Channel'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "df_t0['chave_temp'] = (\n",
    "    df_t0['UID'].astype(str) + '|' + \n",
    "    df_t0['Platform_Name'].astype(str) + '|' + \n",
    "    df_t0['Package_normalizado'].fillna('').astype(str) + '|' + \n",
    "    df_t0['Channel'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "# Comparar n√∫mero de linhas por chave: t0 vs Grupo1\n",
    "linhas_t0_por_chave = df_t0.groupby('chave_temp').size()\n",
    "linhas_g1_por_chave = df_grupo1.groupby('chave_temp').size()\n",
    "\n",
    "# Chaves que aumentaram\n",
    "comparacao = pd.DataFrame({\n",
    "    't0': linhas_t0_por_chave,\n",
    "    'grupo1': linhas_g1_por_chave\n",
    "}).fillna(0)\n",
    "\n",
    "comparacao['diferenca'] = comparacao['grupo1'] - comparacao['t0']\n",
    "chaves_aumentaram = comparacao[comparacao['diferenca'] > 0].sort_values('diferenca', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Chaves que AUMENTARAM linhas: {len(chaves_aumentaram):,}\")\n",
    "print(f\"üìä Total de linhas extras criadas: {chaves_aumentaram['diferenca'].sum():,.0f}\")\n",
    "\n",
    "print(\"\\nüî¥ Top 10 chaves com mais linhas extras:\")\n",
    "for idx, (chave, row) in enumerate(chaves_aumentaram.head(10).iterrows(), 1):\n",
    "    print(f\"\\n{idx}. Chave: {chave[:80]}...\")\n",
    "    print(f\"   t0: {row['t0']:.0f} linhas ‚Üí Grupo1: {row['grupo1']:.0f} linhas (+ {row['diferenca']:.0f})\")\n",
    "    \n",
    "    # Mostrar per√≠odos da t0\n",
    "    print(\"   Per√≠odos na t0:\")\n",
    "    periodos_t0 = df_t0[df_t0['chave_temp'] == chave][['First_Detected', 'Last_Detected']]\n",
    "    for _, p in periodos_t0.iterrows():\n",
    "        first_str = p['First_Detected'].strftime('%d/%m/%Y') if pd.notna(p['First_Detected']) else 'vazio'\n",
    "        last_str = p['Last_Detected'].strftime('%d/%m/%Y') if pd.notna(p['Last_Detected']) else 'vazio'\n",
    "        print(f\"      {first_str} ‚Üí {last_str}\")\n",
    "    \n",
    "    # Mostrar per√≠odos no Grupo1\n",
    "    print(\"   Per√≠odos no Grupo1:\")\n",
    "    periodos_g1 = df_grupo1[df_grupo1['chave_temp'] == chave][['First_Detected', 'Last_Detected']].head(15)\n",
    "    for _, p in periodos_g1.iterrows():\n",
    "        print(f\"      {p['First_Detected']} ‚Üí {p['Last_Detected']}\")\n",
    "\n",
    "# Limpar\n",
    "df_grupo1 = df_grupo1.drop(columns=['chave_temp'])\n",
    "df_t0 = df_t0.drop(columns=['chave_temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65e264c0-f3f8-4bde-9239-e6e3375f743b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîÑ PROCESSANDO GRUPO 2: OBRAS NOVAS (N√ÉO ESTAVAM NA t0)\n",
      "======================================================================\n",
      "üöÄ Executando query DuckDB para obras novas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a3dab59de3460d9d5bc9067d579ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Grupo 2 processado: 1,020,718 linhas geradas\n",
      "   üìä Chaves √∫nicas novas: 774,447\n",
      "   üìä UIDs √∫nicos novos: 295,322\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 8: Processamento Grupo 2 - Obras Novas (OTIMIZADO COM DUCKDB)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÑ PROCESSANDO GRUPO 2: OBRAS NOVAS (N√ÉO ESTAVAM NA t0)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Query SQL para processar Grupo 2\n",
    "query_grupo2 = \"\"\"\n",
    "WITH \n",
    "-- 1. Chaves que est√£o nos Parquets mas N√ÉO na t0\n",
    "chaves_novas AS (\n",
    "    SELECT DISTINCT p.chave\n",
    "    FROM df_parquets_reg p\n",
    "    LEFT JOIN df_t0_reg t ON p.chave = t.chave\n",
    "    WHERE t.chave IS NULL\n",
    "),\n",
    "\n",
    "-- 2. Todas as apari√ß√µes das chaves novas\n",
    "aparicoes_novas AS (\n",
    "    SELECT \n",
    "        p.chave,\n",
    "        p.UID,\n",
    "        p.Platform_Name,\n",
    "        p.Package_normalizado as Package,\n",
    "        p.Channel,\n",
    "        p.data_ref\n",
    "    FROM df_parquets_reg p\n",
    "    INNER JOIN chaves_novas cn ON p.chave = cn.chave\n",
    "),\n",
    "\n",
    "-- 3. Detectar gaps usando window function\n",
    "gaps_detectados AS (\n",
    "    SELECT \n",
    "        chave,\n",
    "        UID,\n",
    "        Platform_Name,\n",
    "        Package,\n",
    "        Channel,\n",
    "        data_ref,\n",
    "        LAG(data_ref) OVER (PARTITION BY chave ORDER BY data_ref) as data_anterior,\n",
    "        DATEDIFF('day', \n",
    "            LAG(data_ref) OVER (PARTITION BY chave ORDER BY data_ref), \n",
    "            data_ref\n",
    "        ) as gap_dias\n",
    "    FROM aparicoes_novas\n",
    "),\n",
    "\n",
    "-- 4. Atribuir per√≠odo baseado em gaps > 22 dias\n",
    "periodos_numerados AS (\n",
    "    SELECT \n",
    "        chave,\n",
    "        UID,\n",
    "        Platform_Name,\n",
    "        Package,\n",
    "        Channel,\n",
    "        data_ref,\n",
    "        SUM(CASE WHEN gap_dias > 22 OR gap_dias IS NULL THEN 1 ELSE 0 END) \n",
    "            OVER (PARTITION BY chave ORDER BY data_ref \n",
    "                  ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as periodo_id\n",
    "    FROM gaps_detectados\n",
    "),\n",
    "\n",
    "-- 5. Agregar por chave + per√≠odo\n",
    "periodos_agregados AS (\n",
    "    SELECT \n",
    "        chave,\n",
    "        periodo_id,\n",
    "        ANY_VALUE(UID) as UID,\n",
    "        ANY_VALUE(Platform_Name) as Platform_Name,\n",
    "        ANY_VALUE(Package) as Package,\n",
    "        ANY_VALUE(Channel) as Channel,\n",
    "        MIN(data_ref) as First_Detected,\n",
    "        MAX(data_ref) as Last_Detected,\n",
    "        COUNT(*) as num_aparicoes\n",
    "    FROM periodos_numerados\n",
    "    GROUP BY chave, periodo_id\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    UID,\n",
    "    Platform_Name,\n",
    "    Package,\n",
    "    Channel,\n",
    "    chave,\n",
    "    First_Detected,\n",
    "    -- Se √∫ltimo per√≠odo est√° recente (>= jul/2025), Last = NULL\n",
    "    CASE \n",
    "        WHEN Last_Detected >= '2025-07-01' THEN NULL\n",
    "        ELSE Last_Detected\n",
    "    END as Last_Detected,\n",
    "    num_aparicoes\n",
    "FROM periodos_agregados\n",
    "ORDER BY chave, First_Detected\n",
    "\"\"\"\n",
    "\n",
    "print(\"üöÄ Executando query DuckDB para obras novas...\")\n",
    "df_grupo2 = con.execute(query_grupo2).fetch_df()\n",
    "\n",
    "print(f\"‚úÖ Grupo 2 processado: {len(df_grupo2):,} linhas geradas\")\n",
    "print(f\"   üìä Chaves √∫nicas novas: {df_grupo2['chave'].nunique():,}\")\n",
    "print(f\"   üìä UIDs √∫nicos novos: {df_grupo2['UID'].nunique():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e128d17-60b4-47e0-b7db-278eda7bed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîó CONSOLIDANDO RESULTADOS\n",
      "======================================================================\n",
      "‚úÖ Total de linhas consolidadas: 1,497,366\n",
      "   üìä UIDs √∫nicos: 341,671\n",
      "   üìä Chaves √∫nicas: 1,103,569\n",
      "‚úÖ Formata√ß√£o final aplicada\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 9: Consolida√ß√£o Final (CORRIGIDA)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîó CONSOLIDANDO RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Juntar Grupo 1 e Grupo 2\n",
    "df_consolidado = pd.concat([df_grupo1, df_grupo2], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Total de linhas consolidadas: {len(df_consolidado):,}\")\n",
    "print(f\"   üìä UIDs √∫nicos: {df_consolidado['UID'].nunique():,}\")\n",
    "print(f\"   üìä Chaves √∫nicas: {df_consolidado['chave'].nunique():,}\")\n",
    "\n",
    "# CORRE√á√ÉO: Converter datas corretamente\n",
    "# Primeiro substituir NaT por None, DEPOIS converter para string\n",
    "df_consolidado['First_Detected'] = df_consolidado['First_Detected'].apply(\n",
    "    lambda x: x.strftime('%d/%m/%Y') if pd.notna(x) else ''\n",
    ")\n",
    "df_consolidado['Last_Detected'] = df_consolidado['Last_Detected'].apply(\n",
    "    lambda x: x.strftime('%d/%m/%Y') if pd.notna(x) else ''\n",
    ")\n",
    "\n",
    "# Remover coluna auxiliar 'chave'\n",
    "if 'chave' in df_consolidado.columns:\n",
    "    df_consolidado = df_consolidado.drop(columns=['chave'])\n",
    "\n",
    "# Remover coluna Package_normalizado se existir\n",
    "if 'Package_normalizado' in df_consolidado.columns:\n",
    "    df_consolidado = df_consolidado.drop(columns=['Package_normalizado'])\n",
    "\n",
    "print(\"‚úÖ Formata√ß√£o final aplicada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26661e29-a0aa-43e2-8c15-1a838c60aa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß REMOVENDO PLATAFORMAS INDESEJADAS\n",
      "======================================================================\n",
      "üö´ Plataformas a excluir: 4\n",
      "   - NBA League Pass\n",
      "   - Archivio Luce\n",
      "   - FIFA+\n",
      "   - DAZN\n",
      "\n",
      "üìä Linhas ANTES do filtro: 1,497,366\n",
      "üìä Linhas DEPOIS do filtro: 1,162,479\n",
      "üìâ Redu√ß√£o: 334,887 linhas (22.4%)\n",
      "\n",
      "‚ö†Ô∏è  AINDA N√ÉO CABE NO EXCEL!\n",
      "   Faltam remover ~113,903 linhas\n",
      "   Considere adicionar mais plataformas √† lista de exclus√£o\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 9.5: FILTRO POR EXCLUS√ÉO DE PLATAFORMAS\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîß REMOVENDO PLATAFORMAS INDESEJADAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Lista de plataformas para EXCLUIR\n",
    "PLATAFORMAS_EXCLUIR = [\n",
    "    'NBA League Pass',\n",
    "    'Archivio Luce',\n",
    "    'FIFA+',\n",
    "    'DAZN'\n",
    "]\n",
    "\n",
    "print(f\"üö´ Plataformas a excluir: {len(PLATAFORMAS_EXCLUIR)}\")\n",
    "for plat in PLATAFORMAS_EXCLUIR:\n",
    "    print(f\"   - {plat}\")\n",
    "\n",
    "# Contar antes\n",
    "linhas_antes = len(df_consolidado)\n",
    "print(f\"\\nüìä Linhas ANTES do filtro: {linhas_antes:,}\")\n",
    "\n",
    "# Aplicar filtro de exclus√£o\n",
    "df_consolidado = df_consolidado[\n",
    "    ~df_consolidado['Platform_Name'].isin(PLATAFORMAS_EXCLUIR)\n",
    "].copy()\n",
    "\n",
    "linhas_depois = len(df_consolidado)\n",
    "print(f\"üìä Linhas DEPOIS do filtro: {linhas_depois:,}\")\n",
    "print(f\"üìâ Redu√ß√£o: {linhas_antes - linhas_depois:,} linhas ({(linhas_antes - linhas_depois)/linhas_antes*100:.1f}%)\")\n",
    "\n",
    "# Verificar se cabe no Excel\n",
    "if linhas_depois > 1048576:\n",
    "    print(f\"\\n‚ö†Ô∏è  AINDA N√ÉO CABE NO EXCEL!\")\n",
    "    print(f\"   Faltam remover ~{linhas_depois - 1048576:,} linhas\")\n",
    "    print(f\"   Considere adicionar mais plataformas √† lista de exclus√£o\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ CABE NO EXCEL! ({linhas_depois:,} < 1,048,576)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83316b9a-d69a-4d50-8c74-213985fab657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç DIAGN√ìSTICO: ANALISANDO DISTRIBUI√á√ÉO DE PER√çODOS\n",
      "======================================================================\n",
      "\n",
      "### DISTRIBUI√á√ÉO DE PER√çODOS POR CHAVE ###\n",
      "count    878008.000000\n",
      "mean          1.323996\n",
      "std           0.536936\n",
      "min           1.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max          37.000000\n",
      "dtype: float64\n",
      "\n",
      "üìä Contagem de per√≠odos (quantas chaves t√™m X per√≠odos):\n",
      "   1 per√≠odo(s): 617,881 chaves (70.4%)\n",
      "   2 per√≠odo(s): 239,268 chaves (27.3%)\n",
      "   3 per√≠odo(s): 18,503 chaves (2.1%)\n",
      "   4 per√≠odo(s): 1,683 chaves (0.2%)\n",
      "   5 per√≠odo(s): 424 chaves (0.0%)\n",
      "   6 per√≠odo(s): 166 chaves (0.0%)\n",
      "   7 per√≠odo(s): 52 chaves (0.0%)\n",
      "   8 per√≠odo(s): 13 chaves (0.0%)\n",
      "   9 per√≠odo(s): 6 chaves (0.0%)\n",
      "   10 per√≠odo(s): 2 chaves (0.0%)\n",
      "   11 per√≠odo(s): 1 chaves (0.0%)\n",
      "   12 per√≠odo(s): 4 chaves (0.0%)\n",
      "   15 per√≠odo(s): 1 chaves (0.0%)\n",
      "   16 per√≠odo(s): 1 chaves (0.0%)\n",
      "   17 per√≠odo(s): 1 chaves (0.0%)\n",
      "   19 per√≠odo(s): 1 chaves (0.0%)\n",
      "   37 per√≠odo(s): 1 chaves (0.0%)\n",
      "\n",
      "‚ö†Ô∏è  CHAVES COM >5 PER√çODOS: 249 (0.0%)\n",
      "\n",
      "Exemplos (top 10 com mais per√≠odos):\n",
      "\n",
      "   üî¥ 37 per√≠odos: fb87ea6d0d52e6a33b908e400915c5b0|Apple TV|TVOD|...\n",
      "      29/01/2021 ‚Üí 28/11/2023\n",
      "      29/01/2021 ‚Üí 28/11/2023\n",
      "      29/01/2021 ‚Üí 28/11/2023\n",
      "      29/01/2021 ‚Üí \n",
      "      29/01/2021 ‚Üí 28/11/2023\n",
      "      29/01/2021 ‚Üí 28/11/2023\n",
      "      29/01/2021 ‚Üí \n",
      "      29/01/2021 ‚Üí \n",
      "      29/01/2021 ‚Üí 28/11/2023\n",
      "      29/01/2021 ‚Üí \n",
      "      29/01/2021 ‚Üí 28/11/2023\n",
      "      29/01/2021 ‚Üí 28/11/2023\n",
      "      29/01/2021 ‚Üí 28/11/2023\n",
      "      29/01/2021 ‚Üí \n",
      "      29/01/2021 ‚Üí 28/11/2023\n",
      "\n",
      "   üî¥ 19 per√≠odos: 74480f4fe0b634dcdbce9b4b3fd52fea|Looke|SVOD|...\n",
      "      09/03/2022 ‚Üí 11/05/2024\n",
      "      09/03/2022 ‚Üí 17/11/2023\n",
      "      09/03/2022 ‚Üí 07/12/2023\n",
      "      09/03/2022 ‚Üí 19/12/2023\n",
      "      09/03/2022 ‚Üí 06/03/2024\n",
      "      09/03/2022 ‚Üí 28/02/2024\n",
      "      09/03/2022 ‚Üí 20/02/2024\n",
      "      09/03/2022 ‚Üí 05/02/2024\n",
      "      09/03/2022 ‚Üí 11/04/2024\n",
      "      09/03/2022 ‚Üí 30/10/2023\n",
      "      09/03/2022 ‚Üí 10/06/2024\n",
      "      09/03/2022 ‚Üí 17/04/2024\n",
      "      09/03/2022 ‚Üí 22/04/2024\n",
      "      09/03/2022 ‚Üí 10/02/2024\n",
      "      09/03/2022 ‚Üí 05/05/2024\n",
      "\n",
      "   üî¥ 17 per√≠odos: a1fe30dfd18da8fe5e59e8c076b72a05|Apple TV|TVOD|...\n",
      "      01/01/2022 ‚Üí 28/11/2023\n",
      "      01/01/2022 ‚Üí \n",
      "      01/01/2022 ‚Üí \n",
      "      01/01/2022 ‚Üí 28/11/2023\n",
      "      01/01/2022 ‚Üí \n",
      "      01/01/2022 ‚Üí 28/11/2023\n",
      "      01/01/2022 ‚Üí \n",
      "      01/01/2022 ‚Üí 28/11/2023\n",
      "      01/01/2022 ‚Üí \n",
      "      01/01/2022 ‚Üí \n",
      "      01/01/2022 ‚Üí 28/11/2023\n",
      "      01/01/2022 ‚Üí 28/11/2023\n",
      "      01/01/2022 ‚Üí \n",
      "      01/01/2022 ‚Üí \n",
      "      01/01/2022 ‚Üí 28/11/2023\n",
      "\n",
      "   üî¥ 16 per√≠odos: 49b1f6155c263109d6c5c1648fed8162|Looke|SVOD|...\n",
      "      14/07/2021 ‚Üí 17/04/2024\n",
      "      14/07/2021 ‚Üí 30/10/2023\n",
      "      14/07/2021 ‚Üí 19/12/2023\n",
      "      14/07/2021 ‚Üí 12/12/2023\n",
      "      14/07/2021 ‚Üí 10/02/2024\n",
      "      14/07/2021 ‚Üí 07/12/2023\n",
      "      14/07/2021 ‚Üí 29/04/2024\n",
      "      14/07/2021 ‚Üí 22/04/2024\n",
      "      14/07/2021 ‚Üí 11/11/2023\n",
      "      14/07/2021 ‚Üí 05/02/2024\n",
      "      14/07/2021 ‚Üí 28/02/2024\n",
      "      14/07/2021 ‚Üí 10/06/2024\n",
      "      14/07/2021 ‚Üí 11/04/2024\n",
      "      14/07/2021 ‚Üí 11/05/2024\n",
      "      14/07/2021 ‚Üí 05/05/2024\n",
      "\n",
      "   üî¥ 15 per√≠odos: f601df9961e35e8b7c0144e64e49a41e|Amazon Prime Video|SVOD|...\n",
      "      25/05/2020 ‚Üí \n",
      "      25/05/2020 ‚Üí 24/02/2022\n",
      "      25/05/2020 ‚Üí 26/05/2022\n",
      "      25/05/2020 ‚Üí 12/09/2022\n",
      "      25/05/2020 ‚Üí 11/04/2024\n",
      "      25/05/2020 ‚Üí 24/02/2022\n",
      "      25/05/2020 ‚Üí 03/04/2023\n",
      "      25/05/2020 ‚Üí 04/04/2024\n",
      "      25/05/2020 ‚Üí 06/10/2022\n",
      "      25/05/2020 ‚Üí 03/10/2023\n",
      "      25/05/2020 ‚Üí 24/01/2022\n",
      "      25/05/2020 ‚Üí \n",
      "      25/05/2020 ‚Üí 02/04/2022\n",
      "      25/05/2020 ‚Üí 17/08/2022\n",
      "      25/05/2020 ‚Üí 25/03/2024\n",
      "\n",
      "   üî¥ 12 per√≠odos: 945241b5ae5c85d2f835922968bdc5e8|Looke|SVOD|...\n",
      "      04/03/2022 ‚Üí 28/02/2024\n",
      "      04/03/2022 ‚Üí 30/11/2023\n",
      "      04/03/2022 ‚Üí 11/04/2024\n",
      "      04/03/2022 ‚Üí 05/02/2024\n",
      "      04/03/2022 ‚Üí 07/12/2023\n",
      "      04/03/2022 ‚Üí 05/05/2024\n",
      "      04/03/2022 ‚Üí 17/04/2024\n",
      "      04/03/2022 ‚Üí 11/11/2023\n",
      "      04/03/2022 ‚Üí 11/05/2024\n",
      "      04/03/2022 ‚Üí 10/06/2024\n",
      "      04/03/2022 ‚Üí 19/01/2024\n",
      "      04/03/2022 ‚Üí 17/11/2023\n",
      "\n",
      "   üî¥ 12 per√≠odos: 8055e6081e775de8586ef3b35a17a5f0|Amazon Prime Video|SVOD|...\n",
      "      04/03/2022 ‚Üí 19/11/2023\n",
      "      04/03/2022 ‚Üí 24/12/2023\n",
      "      04/03/2022 ‚Üí 19/11/2023\n",
      "      04/03/2022 ‚Üí 19/11/2023\n",
      "      04/03/2022 ‚Üí 19/11/2023\n",
      "      04/03/2022 ‚Üí 19/11/2023\n",
      "      04/03/2022 ‚Üí 28/11/2023\n",
      "      04/03/2022 ‚Üí 19/11/2023\n",
      "      04/03/2022 ‚Üí 12/01/2024\n",
      "      04/03/2022 ‚Üí 19/11/2023\n",
      "      04/03/2022 ‚Üí 19/11/2023\n",
      "      04/03/2022 ‚Üí 17/01/2024\n",
      "\n",
      "   üî¥ 12 per√≠odos: 5579c6f3b0d8dc14a6108e2648dd9e22|Looke|SVOD|...\n",
      "      29/01/2021 ‚Üí 05/05/2024\n",
      "      29/01/2021 ‚Üí 17/11/2023\n",
      "      29/01/2021 ‚Üí 10/06/2024\n",
      "      29/01/2021 ‚Üí 01/07/2024\n",
      "      29/01/2021 ‚Üí 19/01/2024\n",
      "      29/01/2021 ‚Üí 07/12/2023\n",
      "      29/01/2021 ‚Üí 22/04/2024\n",
      "      29/01/2021 ‚Üí 29/04/2024\n",
      "      29/01/2021 ‚Üí 17/04/2024\n",
      "      29/01/2021 ‚Üí 05/02/2024\n",
      "      29/01/2021 ‚Üí 11/04/2024\n",
      "      29/01/2021 ‚Üí 11/05/2024\n",
      "\n",
      "   üî¥ 12 per√≠odos: fb50b629df3c501c037e6443e5d6b4d8|Looke|SVOD|...\n",
      "      05/04/2023 ‚Üí 30/10/2023\n",
      "      05/04/2023 ‚Üí 19/01/2024\n",
      "      05/04/2023 ‚Üí 29/04/2024\n",
      "      05/04/2023 ‚Üí 17/04/2024\n",
      "      05/04/2023 ‚Üí 11/05/2024\n",
      "      05/04/2023 ‚Üí 28/02/2024\n",
      "      05/04/2023 ‚Üí 24/11/2023\n",
      "      05/04/2023 ‚Üí 19/12/2023\n",
      "      05/04/2023 ‚Üí 25/01/2024\n",
      "      05/04/2023 ‚Üí 22/04/2024\n",
      "      05/04/2023 ‚Üí 10/02/2024\n",
      "      05/04/2023 ‚Üí 06/03/2024\n",
      "\n",
      "   üî¥ 11 per√≠odos: a9768d92575040b479bdae7b47d6fcdf|Apple TV|TVOD|...\n",
      "      25/05/2020 ‚Üí \n",
      "      25/05/2020 ‚Üí \n",
      "      25/05/2020 ‚Üí \n",
      "      25/05/2020 ‚Üí \n",
      "      25/05/2020 ‚Üí 28/11/2023\n",
      "      25/05/2020 ‚Üí 28/11/2023\n",
      "      25/05/2020 ‚Üí 28/11/2023\n",
      "      25/05/2020 ‚Üí 28/11/2023\n",
      "      25/05/2020 ‚Üí \n",
      "      25/05/2020 ‚Üí 28/11/2023\n",
      "      25/05/2020 ‚Üí 28/11/2023\n",
      "\n",
      "### AN√ÅLISE TEMPORAL ###\n",
      "Obras ABERTAS (Last vazio):       579,900 (49.9%)\n",
      "Obras FECHADAS (Last preenchido): 582,579 (50.1%)\n",
      "\n",
      "### ORIGEM DOS DADOS ###\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA DE DIAGN√ìSTICO: AN√ÅLISE DE PER√çODOS\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç DIAGN√ìSTICO: ANALISANDO DISTRIBUI√á√ÉO DE PER√çODOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Recriar chave temporariamente\n",
    "df_consolidado['chave_temp'] = (\n",
    "    df_consolidado['UID'].astype(str) + '|' + \n",
    "    df_consolidado['Platform_Name'].astype(str) + '|' + \n",
    "    df_consolidado['Package'].fillna('').astype(str) + '|' + \n",
    "    df_consolidado['Channel'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "# 1. Quantos per√≠odos por chave?\n",
    "periodos_por_chave = df_consolidado.groupby('chave_temp').size()\n",
    "\n",
    "print(\"\\n### DISTRIBUI√á√ÉO DE PER√çODOS POR CHAVE ###\")\n",
    "print(periodos_por_chave.describe())\n",
    "print(\"\\nüìä Contagem de per√≠odos (quantas chaves t√™m X per√≠odos):\")\n",
    "contagem = periodos_por_chave.value_counts().sort_index().head(20)\n",
    "for num_periodos, qtd_chaves in contagem.items():\n",
    "    print(f\"   {num_periodos} per√≠odo(s): {qtd_chaves:,} chaves ({qtd_chaves/len(periodos_por_chave)*100:.1f}%)\")\n",
    "\n",
    "# 2. Chaves com MUITOS per√≠odos (poss√≠vel problema)\n",
    "chaves_suspeitas = periodos_por_chave[periodos_por_chave > 5]\n",
    "if len(chaves_suspeitas) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  CHAVES COM >5 PER√çODOS: {len(chaves_suspeitas):,} ({len(chaves_suspeitas)/len(periodos_por_chave)*100:.1f}%)\")\n",
    "    print(\"\\nExemplos (top 10 com mais per√≠odos):\")\n",
    "    for chave, count in chaves_suspeitas.sort_values(ascending=False).head(10).items():\n",
    "        print(f\"\\n   üî¥ {count} per√≠odos: {chave[:80]}...\")\n",
    "        \n",
    "        # Mostrar as datas desses per√≠odos\n",
    "        periodos_exemplo = df_consolidado[df_consolidado['chave_temp'] == chave][\n",
    "            ['UID', 'Platform_Name', 'First_Detected', 'Last_Detected']\n",
    "        ].head(15)\n",
    "        for idx, row in periodos_exemplo.iterrows():\n",
    "            print(f\"      {row['First_Detected']} ‚Üí {row['Last_Detected']}\")\n",
    "\n",
    "# 3. An√°lise de obras abertas vs fechadas\n",
    "print(\"\\n### AN√ÅLISE TEMPORAL ###\")\n",
    "total = len(df_consolidado)\n",
    "abertas = (df_consolidado['Last_Detected'] == '').sum()\n",
    "fechadas = total - abertas\n",
    "print(f\"Obras ABERTAS (Last vazio):       {abertas:,} ({abertas/total*100:.1f}%)\")\n",
    "print(f\"Obras FECHADAS (Last preenchido): {fechadas:,} ({fechadas/total*100:.1f}%)\")\n",
    "\n",
    "# 4. Contagem por grupo de origem\n",
    "print(\"\\n### ORIGEM DOS DADOS ###\")\n",
    "if 'origem_grupo' in df_consolidado.columns:\n",
    "    print(df_consolidado['origem_grupo'].value_counts())\n",
    "\n",
    "# Remover chave tempor√°ria\n",
    "df_consolidado = df_consolidado.drop(columns=['chave_temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b14b47-dc73-4575-9867-b2865619beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 10: Export para PARQUET e EXCEL\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üíæ EXPORTANDO RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Gerar nome base com timestamp\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "nome_base = f'Base_Consolidada_t0_t1_{timestamp}'\n",
    "\n",
    "# 1. SALVAR PARQUET (sempre funciona, sem limite de linhas)\n",
    "arquivo_parquet = PASTA_OUTPUT / f'{nome_base}.parquet'\n",
    "df_consolidado.to_parquet(arquivo_parquet, index=False)\n",
    "print(f\"‚úÖ Parquet salvo: {arquivo_parquet}\")\n",
    "print(f\"   üìä Dimens√µes: {len(df_consolidado):,} linhas x {len(df_consolidado.columns)} colunas\")\n",
    "\n",
    "# 2. SALVAR EXCEL (se couber)\n",
    "arquivo_excel = PASTA_OUTPUT / f'{nome_base}.xlsx'\n",
    "\n",
    "if len(df_consolidado) <= 1048576:\n",
    "    df_consolidado.to_excel(arquivo_excel, index=False)\n",
    "    print(f\"‚úÖ Excel salvo: {arquivo_excel}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Excel N√ÉO salvo - arquivo muito grande ({len(df_consolidado):,} linhas)\")\n",
    "    print(f\"   Arquivo Parquet dispon√≠vel: {arquivo_parquet}\")\n",
    "    print(f\"   Adicione mais plataformas √† exclus√£o na c√©lula 9.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe19b92-5a24-4190-b2d6-fbb9cfdb8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# C√âLULA 11: Relat√≥rio de Auditoria\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RELAT√ìRIO DE AUDITORIA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n### ORIGEM DOS DADOS ###\")\n",
    "print(f\"Grupo 1 (da t0):     {len(df_grupo1):,} linhas\")\n",
    "print(f\"Grupo 2 (novas):     {len(df_grupo2):,} linhas\")\n",
    "print(f\"TOTAL:               {len(df_consolidado):,} linhas\")\n",
    "\n",
    "print(\"\\n### AN√ÅLISE TEMPORAL ###\")\n",
    "obras_abertas = (df_consolidado['Last_Detected'] == '').sum()\n",
    "obras_fechadas = (df_consolidado['Last_Detected'] != '').sum()\n",
    "print(f\"Obras ABERTAS (Last vazio):    {obras_abertas:,} ({obras_abertas/len(df_consolidado)*100:.1f}%)\")\n",
    "print(f\"Obras FECHADAS (Last preenchido): {obras_fechadas:,} ({obras_fechadas/len(df_consolidado)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n### DISTRIBUI√á√ÉO POR PLATAFORMA (Top 10) ###\")\n",
    "print(df_consolidado['Platform_Name'].value_counts().head(10))\n",
    "\n",
    "print(\"\\n### DISTRIBUI√á√ÉO POR PACKAGE ###\")\n",
    "print(df_consolidado['Package'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"‚úÖ PROCESSAMENTO CONCLU√çDO EM: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Fechar conex√£o DuckDB\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794dd7ab-9c2d-435c-a309-ffe6b8b53e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
